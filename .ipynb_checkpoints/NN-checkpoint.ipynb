{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e4666-52ab-4099-a2bd-bf92758c43e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.optimizers import Adam, AdamW, Nadam, Adamax, SGD, RMSprop\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Normalization, BatchNormalization, Activation\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17cb26b-6eb2-4a19-941a-40c13d8947c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 28112025\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "ERROR_MARGIN_G = 2\n",
    "\n",
    "BATCH_SIZE=256\n",
    "MAX_TRIALS=20\n",
    "EPOCHS=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2e5cf-00b5-4501-8317-fb3c058b78d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.read_csv('3d_print_miniatures_base.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c6ab50-b7fd-4bd3-a1ff-2252575807b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dfdd49-9f9c-4a68-ba75-07cf2c9e585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab32ed8-cf05-44ec-947e-488e8813d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- ARTISTS ({df['artist'].nunique()})---\")\n",
    "print(sorted(df['artist'].unique()))\n",
    "\n",
    "print()\n",
    "print(f\"--- MINIS ({df['mini'].nunique()})---\")\n",
    "minis = sorted(df['mini'].unique())\n",
    "print(minis[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4158ea-27ed-451c-9297-bce16c610b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "df[\"artist\"].value_counts().plot(kind=\"bar\")\n",
    "plt.xlabel(\"Artist\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.title(\"Distribution of Artists\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4539c73-c9e8-48fb-8f22-27ac4da29293",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_per_mini = df.groupby(\"mini\").size()\n",
    "\n",
    "avg_parts = parts_per_mini.mean()\n",
    "max_parts = parts_per_mini.max()\n",
    "min_parts = parts_per_mini.min()\n",
    "\n",
    "print(\"Average parts per mini:\", avg_parts)\n",
    "print(\"Max parts for a mini:\", max_parts)\n",
    "print(\"Min parts for a mini:\", min_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cbe3b5-1c53-4853-abfb-062ef1aa2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nan_columns(df):\n",
    "    nan_rows = None\n",
    "    if df.isnull().values.any():\n",
    "        print(\"Dataset contains NaN values.\")\n",
    "    \n",
    "        nan_counts = df.isnull().sum()\n",
    "        print(\"NaN counts in each column:\")\n",
    "        print(nan_counts[nan_counts > 0])\n",
    "\n",
    "        print()\n",
    "        \n",
    "        nan_rows = df[df.isnull().any(axis=1)]\n",
    "\n",
    "    return nan_rows\n",
    "\n",
    "df_nans = is_nan_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671cbc4d-5045-4602-848c-e278e480c266",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['weight'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078f45e-8f2d-43a3-9962-55a16ac56b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df, features_to_keep, label):\n",
    "    df['kb']       = df['kb'].astype(int)\n",
    "    df['volume']   = np.ceil(df['volume'] * 10) / 10\n",
    "\n",
    "    # Polynomial Features\n",
    "    df['volume_g'] = df['volume'] * 1e-3 * 1.1\n",
    "    df['mass_g'] = df['mass']  * 1e-3 * 1.1\n",
    "    \n",
    "    # Interaction Features\n",
    "    df['volume_mass_interaction'] = df['volume_g'] * df['mass_g']\n",
    "    df['surface_volume_ratio'] = df['surface_area'] / df['volume']\n",
    "    df['bbox_volume_ratio'] = df['bbox_area'] / df['volume']\n",
    "    \n",
    "    # Dimension Ratios\n",
    "    df['surface_mass_ratio'] = df['surface_area'] / df['mass']\n",
    "    df['bbox_mass_ratio'] = df['bbox_area'] / df['mass']\n",
    "\n",
    "    for col in ['surface_area', 'bbox_x', 'bbox_y', 'bbox_z', 'bbox_area', 'weight', 'scale',\n",
    "                'volume', 'volume_g', 'mass', 'mass_g', 'volume_mass_interaction', 'surface_volume_ratio', 'surface_mass_ratio']:\n",
    "        df[col] = df[col].round(1)\n",
    "\n",
    "    df = df[[*features_to_keep, label]]\n",
    "\n",
    "    return df\n",
    "\n",
    "features = [\n",
    "    'artist',\n",
    "    'volume',\n",
    "    'mass',\n",
    "    'scale',\n",
    "    'bbox_x',\n",
    "    'bbox_y',\n",
    "    'bbox_z',\n",
    "    'bbox_area',\n",
    "    'surface_area',\n",
    "    'volume_mass_interaction',\n",
    "    'surface_volume_ratio',\n",
    "    'surface_mass_ratio'\n",
    "]\n",
    "\n",
    "label = 'weight'\n",
    "\n",
    "df = preprocess_df(df, features, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4b6e95-6f66-4d77-93e6-57159beab153",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992ca6e6-0c8e-4c61-89fb-30f9cf867986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distribution_with_bounds(df, feature=\"volume\", q1=0.25, q3=0.75, factor=1.5, bins=100,):\n",
    "    col = df[feature]\n",
    "    Q1 = col.quantile(q1)\n",
    "    Q3 = col.quantile(q3)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - factor * IQR\n",
    "    upper_bound = Q3 + factor * IQR\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(col, bins=bins)\n",
    "    plt.axvline(lower_bound, linestyle=\"--\")\n",
    "    plt.axvline(upper_bound, linestyle=\"--\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.title(f\"{feature} distribution with IQR bounds\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"{feature} Q1={Q1:.3f}, Q3={Q3:.3f}, IQR={IQR:.3f}\")\n",
    "    print(f\"Bounds: [{lower_bound:.3f}, {upper_bound:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538da06-4c8c-4cd3-ab10-23978368e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution_with_bounds(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61079371-1ff9-4844-9c93-5ef6e5893088",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = df['artist'].unique()\n",
    "artist_datasets = {artist: df[df['artist'] == artist].copy() for artist in artists}\n",
    "\n",
    "df_1 = artist_datasets[0]\n",
    "df_2  = artist_datasets[1]\n",
    "df_3  = artist_datasets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6ef96-4852-4430-8ebe-7349312f293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, quantile_1=0.25, quantile_3=0.85):\n",
    "    Q1 = df.quantile(quantile_1)\n",
    "    Q3 = df.quantile(quantile_3)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    outlier_criteria = df > (Q3 + 1.5 * IQR)\n",
    "    \n",
    "    return df[~(outlier_criteria).any(axis=1)]\n",
    "\n",
    "\n",
    "print('Shapes before removing outliers')\n",
    "print(df.shape)\n",
    "\n",
    "df = remove_outliers(df)\n",
    "\n",
    "print('\\nShapes after removing outliers')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a27608-4601-44e2-ae71-801f5718df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape, df_1.shape, df_2.shape, df_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd511ea5-9eb4-4d25-b5bd-8d47875eecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, target, test_size, seed, stratified=False, stratified_column=None, num_bins=4):\n",
    "    if stratified and stratified_column:\n",
    "        df['stratified_bins'] = pd.qcut(df[stratified_column], q=num_bins, duplicates='drop')\n",
    "        stratify_col = df['stratified_bins']\n",
    "    else:\n",
    "        stratify_col = None\n",
    "\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=stratify_col\n",
    "    )\n",
    "\n",
    "    if stratified and stratified_column:\n",
    "        X_train = X_train.drop('stratified_bins', axis=1)\n",
    "        X_test = X_test.drop('stratified_bins', axis=1)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_dataset(df, 'weight', 0.2, SEED, stratified=True, stratified_column='volume')\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = split_dataset(df_1, 'weight', 0.2, SEED, stratified=True, stratified_column='volume')\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = split_dataset(df_2, 'weight', 0.2, SEED, stratified=True, stratified_column='volume')\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = split_dataset(df_3, 'weight', 0.2, SEED, stratified=True, stratified_column='volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b0faae-d273-4a61-aa75-b8573b5061f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "X_train_1 = np.array(X_train_1, dtype=np.float32)\n",
    "X_train_2 = np.array(X_train_2, dtype=np.float32)\n",
    "X_train_3 = np.array(X_train_3, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a391c2-7d4b-4f1a-8510-5fa52c08e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model(hp, n_features, normalizer):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=(n_features,)))\n",
    "    model.add(normalizer)\n",
    "\n",
    "    init_units = hp.Int('init_units', min_value=32, max_value=1024, step=32)\n",
    "\n",
    "    regularizer = hp.Choice('regularizer', values=['none', 'l1', 'l2', 'both'])\n",
    "    l1_str = hp.Float('l1_str', min_value=1e-6, max_value=1e-2, sampling='log')\n",
    "    l2_str = hp.Float('l2_str', min_value=1e-6, max_value=1e-2, sampling='log')\n",
    "\n",
    "    tested_activations = ['relu', 'leaky_relu', 'elu', 'tanh', 'selu', 'linear', 'swish', 'mish']\n",
    "    \n",
    "    if regularizer == 'none':\n",
    "        kernel_regularizer = None\n",
    "    elif regularizer == 'l1':\n",
    "        kernel_regularizer = l1(l1_str)\n",
    "    elif regularizer == 'l2':\n",
    "        kernel_regularizer = l2(l2_str)\n",
    "    else:\n",
    "        kernel_regularizer = l1_l2(l1=l1_str, l2=l2_str)\n",
    "    \n",
    "    model.add(Dense(init_units, kernel_regularizer=kernel_regularizer))\n",
    "    model.add(Activation(hp.Choice('init_activation', values=tested_activations)))\n",
    "\n",
    "    if hp.Choice('init_batch_normalization', values=[True, False]):\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    num_dense_layers = hp.Int('num_dense_layers', min_value=1, max_value=8, step=1)\n",
    "    num_units = [\n",
    "        hp.Int(f'units_{i}', min_value=32, max_value=1024, step=32)\n",
    "        for i in range(num_dense_layers)\n",
    "    ]\n",
    "\n",
    "    for i in range(num_dense_layers):\n",
    "        normalization = hp.Choice(f'batch_normalization_{i}', values=[True, False])\n",
    "        normalization_location = hp.Choice(\n",
    "            f'batch_normalization_position_{i}', values=['before', 'after']\n",
    "        )\n",
    "        \n",
    "        model.add(Dense(num_units[i], kernel_regularizer=kernel_regularizer))\n",
    "\n",
    "        if normalization and normalization_location == 'before':\n",
    "            model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Activation(hp.Choice(f'activation_{i}', values=tested_activations)))\n",
    "\n",
    "        if normalization and normalization_location == 'after':\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "        dropout_units = hp.Float(\n",
    "            f'dropout_{i}', min_value=0.0, max_value=0.75, step=0.05\n",
    "        )\n",
    "\n",
    "        if dropout_units > 0:\n",
    "            model.add(Dropout(dropout_units))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    opt_name = hp.Choice('optimizer', values=['adam', 'adamw', 'nadam', 'adamax', 'sgd', 'rmsprop'])\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-6, max_value=1e-2, sampling='log')\n",
    "\n",
    "    if opt_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif opt_name == 'adamw':\n",
    "        optimizer = AdamW(learning_rate=learning_rate)\n",
    "    elif opt_name == 'nadam':\n",
    "        optimizer = Nadam(learning_rate=learning_rate)\n",
    "    elif opt_name == 'adamax':\n",
    "        optimizer = Adamax(learning_rate=learning_rate)\n",
    "    elif opt_name == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=[MeanAbsoluteError()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def search_best_model(project_name, x, y, seed, epochs=50, max_trials=10, split=0.2, batch_size=None, show_model=True):\n",
    "    n_features = x.shape[1]\n",
    "\n",
    "    normalizer = Normalization(axis=-1)\n",
    "    normalizer.adapt(x)\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        lambda hp: build_and_compile_model(hp, n_features, normalizer),\n",
    "        objective=kt.Objective(\"val_mean_absolute_error\", direction=\"min\"),\n",
    "        max_trials=max_trials,\n",
    "        directory='bayesian_optimization',\n",
    "        project_name=project_name,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    tuner.search(\n",
    "        x, y,\n",
    "        epochs=epochs,\n",
    "        validation_split=split,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor='val_mean_absolute_error', min_delta=0.01, patience=10,\n",
    "                          verbose=0, mode='min', restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_mean_absolute_error', factor=0.2, patience=5,\n",
    "                              min_lr=1e-8, mode='min', verbose=0)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    best_model_from_tuner = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "    if show_model:\n",
    "        print(\"Best Model from Tuner:\")\n",
    "        best_model_from_tuner.summary()\n",
    "\n",
    "    return best_hps, best_model_from_tuner\n",
    "\n",
    "def show_best_hps(hps):\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    for hp in hps.values:\n",
    "        print(f\"{hp}: {hps.get(hp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dbbf14-993d-4f30-98be-3105eee4e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps, best_model = search_best_model('All_Artists', X_train, y_train, epochs=EPOCHS, max_trials=MAX_TRIALS, seed=SEED, batch_size=BATCH_SIZE)\n",
    "show_best_hps(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e6948b-93fb-48be-be0d-61a17e11a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps_1, best_model_1 = search_best_model('Artist_0', X_train_1, y_train_1, epochs=EPOCHS, max_trials=MAX_TRIALS, seed=SEED, batch_size=BATCH_SIZE)\n",
    "show_best_hps(best_hps_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cbe02b-7d8b-43bd-81f8-e1f7f1524340",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps_2, best_model_2 = search_best_model('Artist_1', X_train_2, y_train_2, epochs=EPOCHS, max_trials=MAX_TRIALS, seed=SEED, batch_size=BATCH_SIZE)\n",
    "show_best_hps(best_hps_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9800c377-92c5-4e4f-bbc0-97ba1c099415",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps_3, best_model_3 = search_best_model('Artist_2', X_train_3, y_train_3, epochs=EPOCHS, max_trials=MAX_TRIALS, seed=SEED, batch_size=BATCH_SIZE)\n",
    "show_best_hps(best_hps_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99866c-fc6e-4cc4-8c4d-1b47176987d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.3,\n",
    "    epochs=100,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_mean_absolute_error', min_delta=0.01, patience=10, mode='min', restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_mean_absolute_error', factor=0.2, patience=5, min_lr=1e-8, mode='min')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb53a1-1943-420c-a1ad-09de0f82fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history_1 = best_model_1.fit(\n",
    "    X_train_1, y_train_1,\n",
    "    validation_split=0.3,\n",
    "    epochs=100,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_mean_absolute_error', min_delta=0.01, patience=10, mode='min', restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_mean_absolute_error', factor=0.2, patience=5, min_lr=1e-8, mode='min')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f96ee6-9669-4964-b2ab-848c8db43ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history_2 = best_model_2.fit(\n",
    "    X_train_2, y_train_2,\n",
    "    validation_split=0.3,\n",
    "    epochs=100,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_mean_absolute_error', min_delta=0.01, patience=10, mode='min', restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_mean_absolute_error', factor=0.2, patience=5, min_lr=1e-8, mode='min')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc134b38-20d1-4838-85f1-c5f061bd0edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history_3 = best_model_3.fit(\n",
    "    X_train_3, y_train_3,\n",
    "    validation_split=0.3,\n",
    "    epochs=100,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_mean_absolute_error', min_delta=0.01, patience=10, mode='min', restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_mean_absolute_error', factor=0.2, patience=5, min_lr=1e-8, mode='min')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d1c2f-3613-4137-88f0-514de09e84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, artist=None):\n",
    "    plt.plot(history.history['mean_absolute_error'], label='mean_absolute_error')\n",
    "    plt.plot(history.history['val_mean_absolute_error'], label='val_mean_absolute_error')\n",
    "    plt.ylim([0, 10])\n",
    "    plt.title(f\"Artist: {artist}\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [MAE]')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(history, 'All')\n",
    "plot_loss(history_1, '0')\n",
    "plot_loss(history_2, '1')\n",
    "plot_loss(history_3, '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89454cc-28f7-46e9-8683-100e4e728c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best All_Artists Model: ', best_model.evaluate(X_test, y_test, verbose=0))\n",
    "print('Best Artist_0 Model: ', best_model_1.evaluate(X_test_1, y_test_1, verbose=0))\n",
    "print('Best Artist_1 Model: ', best_model_2.evaluate(X_test_2, y_test_2, verbose=0))\n",
    "print('Best Artist_2 Model: ', best_model_3.evaluate(X_test_3, y_test_3, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f0292-db57-4919-8a8d-e73fd0bb2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(X_test).flatten()\n",
    "predictions_1 = best_model_1.predict(X_test_1).flatten()\n",
    "predictions_2 = best_model_2.predict(X_test_2).flatten()\n",
    "predictions_3 = best_model_3.predict(X_test_3).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e8d022-83e1-4810-8504-d86973f8113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(y, predictions, error_margin, artist=None):\n",
    "    residuals = y - predictions.flatten()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Drawing two horizontal lines across the plot for the global error margin\n",
    "    # These lines are drawn at +/- error_margin directly\n",
    "    plt.axhline(y=error_margin, color='lightgrey', linestyle='--', alpha=0.8, label=f'+/- {error_margin} error margin')\n",
    "    plt.axhline(y=-error_margin, color='lightgrey', linestyle='--', alpha=0.8)\n",
    "    \n",
    "    plt.scatter(predictions, residuals, alpha=0.5, label='Residuals')\n",
    "    plt.axhline(y=0, color='r', linestyle='--', label='Zero Residual')\n",
    "    \n",
    "    # Setting the y-axis limits to ensure the error margin lines are within view\n",
    "    # Expand the limits based on the actual residuals and the error margin\n",
    "    plt.ylim(min(residuals.min(), -error_margin * 1.1), max(residuals.max(), error_margin * 1.1))\n",
    "    \n",
    "    plt.title(f'Residuals vs. Predicted Values - Artist: {artist}')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_residuals(y_test, predictions, ERROR_MARGIN_G, artist='All')\n",
    "plot_residuals(y_test_1, predictions_1, ERROR_MARGIN_G, artist='0')\n",
    "plot_residuals(y_test_2, predictions_2, ERROR_MARGIN_G, artist='1')\n",
    "plot_residuals(y_test_3, predictions_3, ERROR_MARGIN_G, artist='2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3630d696-9869-4f66-9ab4-97c9894b9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_margin(y, predictions, error_margin, artist=None):\n",
    "    # Direct comparison with the error margin in grams\n",
    "    residuals = np.abs(y - predictions)\n",
    "    accurate_predictions = residuals <= error_margin\n",
    "    underestimations = (y - predictions) > error_margin\n",
    "    overestimations = (predictions - y) > error_margin\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(predictions[accurate_predictions], y[accurate_predictions], color='green', alpha=0.5, label=f'Accurate within {error_margin} grams')\n",
    "    plt.scatter(predictions[underestimations], y[underestimations], color='blue', alpha=0.5, label='Underestimations')\n",
    "    plt.scatter(predictions[overestimations], y[overestimations], color='red', alpha=0.5, label='Overestimations')\n",
    "    \n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('True Values')\n",
    "    plt.title(f'Prediction Accuracy with {error_margin} Grams Error Margin - Artist: {artist}')\n",
    "    plt.legend()\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)  # Diagonal line for reference\n",
    "    plt.show()\n",
    "\n",
    "plot_error_margin(y_test, predictions, ERROR_MARGIN_G, artist='All')\n",
    "plot_error_margin(y_test_1, predictions_1, ERROR_MARGIN_G, artist='0')\n",
    "plot_error_margin(y_test_2, predictions_2, ERROR_MARGIN_G, artist='1')\n",
    "plot_error_margin(y_test_3, predictions_3, ERROR_MARGIN_G, artist='2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63808e27-5916-4386-bcb1-cc3ec38d510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_distribution(y_true, y_pred, artist=None):\n",
    "    differences = y_pred - y_true\n",
    "\n",
    "    boundaries = [(-np.inf, -15), (-15, -10), (-10, -5), (-5, -4), (-4, -3), (-3, -2), (-2, -1), (-1, 1),\n",
    "                  (1, 2), (2, 3), (3, 4), (4, 5), (5, 10), (10, 15), (15, np.inf)]\n",
    "    labels = ['<-15', '-15 to -10', '-10 to -5', '-5 to -4', '-4 to -3', '-3 to -2', '-2 to -1', \n",
    "              'Within +/- 1', \n",
    "              '1 to 2', '2 to 3', '3 to 4', '4 to 5', '5 to 10', '10 to 15', '>15']\n",
    "    \n",
    "    counts = np.zeros(len(labels))\n",
    "    \n",
    "    for i, (lower, upper) in enumerate(boundaries):\n",
    "        if i == 7:  # Special case for 'Within +/- 1'\n",
    "            counts[i] = np.sum((differences >= lower) & (differences < upper))\n",
    "        else:\n",
    "            counts[i] = np.sum((differences > lower) & (differences <= upper))\n",
    "    \n",
    "    non_zero_counts = counts > 0\n",
    "    filtered_labels = np.array(labels)[non_zero_counts]\n",
    "    filtered_counts = counts[non_zero_counts]\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.bar(filtered_labels, filtered_counts, color='skyblue')\n",
    "    plt.xlabel('Error Range (grams)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Prediction Error Distribution - Artist: {artist}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--')\n",
    "    plt.show()\n",
    "\n",
    "plot_error_distribution(y_test, predictions, artist='All')\n",
    "plot_error_distribution(y_test_1, predictions_1, artist='0')\n",
    "plot_error_distribution(y_test_2, predictions_2, artist='1')\n",
    "plot_error_distribution(y_test_3, predictions_3, artist='2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d892e957-c539-4e4d-9fed-69b21f9ed661",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison = pd.DataFrame({'y_test':y_test, 'y_pred': predictions})\n",
    "df_1_comparison = pd.DataFrame({'y_test':y_test_1, 'y_pred': predictions_1})\n",
    "df_2_comparison = pd.DataFrame({'y_test':y_test_2, 'y_pred': predictions_2})\n",
    "df_3_comparison = pd.DataFrame({'y_test':y_test_3, 'y_pred': predictions_3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdffd6c8-96ec-4fb6-a81e-441a69efc7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model_name, predictions, y, error_margin=2, num_predictors=None):\n",
    "    if num_predictors is None:\n",
    "        raise ValueError(\"num_predictors must be provided to calculate adjusted R-squared.\")\n",
    "    \n",
    "    n = len(y)\n",
    "    p = num_predictors\n",
    "    \n",
    "    absolute_diff = np.abs(predictions - y)\n",
    "    correct_predictions = np.sum(absolute_diff <= error_margin) / n * 100\n",
    "    \n",
    "    rmse = mean_squared_error(y, predictions)\n",
    "    mae = mean_absolute_error(y, predictions)\n",
    "    r_squared = r2_score(y, predictions)\n",
    "    adjusted_r_squared = 1 - (1-r_squared) * (n-1) / (n-p-1)\n",
    "\n",
    "    print(f'++++++++ Results for model {model_name} ++++++++\\n')\n",
    "    print(f\"Percentage of predictions within {error_margin} grams of the actual values: {correct_predictions}%\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"R-squared: {r_squared}\")\n",
    "    print(f\"Adjusted R-squared: {adjusted_r_squared}\")\n",
    "\n",
    "    return correct_predictions\n",
    "\n",
    "class KerasRegressorWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y, verbose=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        results = self.model.evaluate(X, y, verbose=0)\n",
    "        # Assuming the first element is the loss\n",
    "        loss = results[0]\n",
    "        return -loss\n",
    "\n",
    "def plot_permutation_importance(model, X_test, y_test, seed, artist=None):\n",
    "    wrapped_model = KerasRegressorWrapper(model)\n",
    "    result = permutation_importance(wrapped_model, X_test, y_test, n_repeats=10, random_state=seed, n_jobs=1)\n",
    "    \n",
    "    sorted_idx = result.importances_mean.argsort()\n",
    "    plt.barh(X_test.columns[sorted_idx], result.importances_mean[sorted_idx])\n",
    "    plt.title(f'Prediction Error Distribution - Artist: {artist}')\n",
    "    plt.xlabel(\"Permutation Importance\")\n",
    "    plt.show()\n",
    "\n",
    "def calculate_differences(y_test, y_pred):\n",
    "    differences = y_pred - y_test\n",
    "    \n",
    "    avg_difference = np.mean(np.abs(differences))\n",
    "    min_difference = np.min(differences)\n",
    "    max_difference = np.max(differences)\n",
    "    \n",
    "    print(f'Average difference: {avg_difference}, Minimum difference: {min_difference}, Maximum difference: {max_difference}')\n",
    "\n",
    "def save_model(model, file_path, accuracy):\n",
    "    if accuracy > 80 and not os.path.exists(file_path):\n",
    "        model.save(file_path)\n",
    "        print(f'Model saved as {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84aa931-9060-48db-9a55-da2623ea8308",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = get_results('All_Artist', predictions, y_test, error_margin=ERROR_MARGIN_G, num_predictors=best_model.input_shape[1])\n",
    "calculate_differences(y_test, predictions)\n",
    "save_model(best_model, f'model_all-{round(accuracy, 3)}.keras', accuracy)\n",
    "plot_permutation_importance(best_model, X_test, y_test, SEED, artist='All')\n",
    "\n",
    "accuracy_1 = get_results('Artist_1', predictions_1, y_test_1, error_margin=ERROR_MARGIN_G, num_predictors=best_model_1.input_shape[1])\n",
    "calculate_differences(y_test_1, predictions_1)\n",
    "save_model(best_model_1, f'model_1-{round(accuracy_1, 3)}.keras', accuracy_1)\n",
    "plot_permutation_importance(best_model_1, X_test_1, y_test_1, SEED, artist='0')\n",
    "\n",
    "accuracy_2 = get_results('Artist_2', predictions_2, y_test_2, error_margin=ERROR_MARGIN_G, num_predictors=best_model_2.input_shape[1])\n",
    "calculate_differences(y_test_2, predictions_2)\n",
    "save_model(best_model_2, f'model_2-{round(accuracy_2, 3)}.keras', accuracy_2)\n",
    "plot_permutation_importance(best_model_2, X_test_2, y_test_2, SEED, artist='1')\n",
    "\n",
    "accuracy_3 = get_results('Artist_3', predictions_3, y_test_3, error_margin=ERROR_MARGIN_G, num_predictors=best_model_3.input_shape[1])\n",
    "calculate_differences(y_test_3, predictions_3)\n",
    "save_model(best_model_3, f'model_3-{round(accuracy_3, 3)}.keras', accuracy_3)\n",
    "plot_permutation_importance(best_model_3, X_test_3, y_test_3, SEED, artist='2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba4cd4-cdf5-45d9-879a-4b9f06e04bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
